{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compendio de Optimización No Lineal:\n",
        "## Teoría Matemática, Análisis de Convergencia y Algoritmos Computacionales\n",
        "\n",
        "**Presentado por:** Ing. Melvin E. Padilla  \n",
        "**Cátedra:** Programación Matemática  \n",
        "**Maestría en Matemática y Computación** \n",
        "**Universidad de Carabobo, Venezuela** \n",
        "**2025**\n",
        "\n",
        "---\n",
        "\n",
        "### Resumen\n",
        "Este documento constituye una recopilación exhaustiva sobre la optimización matemática no lineal sin restricciones. Integra los fundamentos topológicos (convexidad, concavidad), el análisis riguroso de convergencia, el Teorema de Taylor multivariable, la teoría de formas cuadráticas y la implementación computacional en Python. Está diseñado para cubrir desde la teoría base hasta la aplicación numérica avanzada necesaria en ingeniería."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Índice\n",
        "\n",
        "1. Introducción y Marco de Referencia\n",
        "2. El Problema Unidimensional\n",
        "   - 2.1. Topología: Unimodalidad, Convexidad y Concavidad\n",
        "   - 2.1.1. Implementación Computacional: Visualización de Dualidad\n",
        "   - 2.2. Caracterización de Óptimos Locales\n",
        "   - 2.3. Métodos de Búsqueda sin Derivadas\n",
        "     - 2.3.1. Método de la Sección Dorada\n",
        "     - 2.3.2. Interpolación Parabólica\n",
        "3. El Problema Multivariable: Fundamentos Matemáticos\n",
        "   - 3.1. El Gradiente y la Hessiana\n",
        "   - 3.2. Teorema de Taylor Multivariable\n",
        "   - 3.3. Formas Cuadráticas y Criterio de Sylvester\n",
        "   - 3.4. Gráficos de Contorno\n",
        "4. Algoritmos de Descenso Multivariable\n",
        "   - 4.1. Análisis de Convergencia\n",
        "   - 4.2. Método del Máximo Descenso (Gradient Descent)\n",
        "   - 4.3. Método de Newton Multivariable\n",
        "5. El Problema del Valle Curvo\n",
        "   - 5.1. Por qué falla el Gradiente (Primer Orden)\n",
        "6. La Solución de Newton (Segundo Orden)\n",
        "   - 6.1. ¿Qué hace la Hessiana realmente?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introducción y Marco de Referencia\n",
        "\n",
        "> **Nota de Cátedra:** El ingeniero de postgrado debe transitar fluidamente entre la abstracción matemática y la implementación numérica. Este documento no es un manual de usuario, sino un compendio teórico-práctico. Las referencias citadas a continuación son la base canónica sobre la cual se construyen solvers comerciales como los de GAMS, MATLAB o Python SciPy.\n",
        "\n",
        "El problema de optimización busca encontrar un vector de variables de diseño que minimice o maximice una función objetivo, sujeto a restricciones o libre de ellas. En ingeniería, esto es fundamental para el diseño de sistemas eficientes, desde estructuras mecánicas hasta procesos químicos.\n",
        "\n",
        "Para el desarrollo riguroso de este contenido, se han tomado como base las siguientes referencias canónicas en el campo de la optimización numérica:\n",
        "\n",
        "1. Nocedal, J., & Wright, S. J. (2006). *Numerical Optimization*. Springer.\n",
        "2. Rao, S. S. (2009). *Engineering Optimization: Theory and Practice*. John Wiley & Sons.\n",
        "3. Luenberger, D. G., & Ye, Y. (2008). *Linear and Nonlinear Programming*. Springer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. El Problema Unidimensional\n",
        "\n",
        "El problema unidimensional busca encontrar un escalar $x^*$ tal que optimice $f(x)$, donde $f: \\mathbb{R} \\to \\mathbb{R}$. Aunque los problemas reales suelen ser multivariables, la optimización unidimensional es crítica porque forma la base de los algoritmos de “Búsqueda de Línea” (Line Search) utilizados en cada iteración de los problemas de n-dimensiones.\n",
        "\n",
        "### 2.1. Topología: Unimodalidad, Convexidad y Concavidad\n",
        "\n",
        "> **Nota de Cátedra:** La dualidad es clave en ingeniería: Maximizar la eficiencia (cóncava) es equivalente a minimizar el desperdicio (convexa). Matemáticamente, maximizar $f(x)$ es idéntico a minimizar $-f(x)$. Entender esta simetría permite usar los mismos algoritmos para ambos propósitos.\n",
        "\n",
        "Para garantizar la existencia y unicidad de soluciones, debemos analizar la topología de la función.\n",
        "\n",
        "**Definición 2.1 (Unimodalidad).** Una función $f(x)$ es unimodal en el intervalo $[a, b]$ si existe un único $x^* \\in [a, b]$ tal que $f(x)$ es monótona decreciente a la izquierda de $x^*$ y creciente a la derecha (para el caso de minimización). Es la condición mínima necesaria para que los métodos de reducción de intervalo funcionen.\n",
        "\n",
        "**Definición 2.2 (Convexidad).** Una función $f$ es convexa si el segmento de línea que une dos puntos cualesquiera de la gráfica queda por encima de la curva. Matemáticamente, para todo $x, y$ en el dominio y $\\lambda \\in [0, 1]$:\n",
        "$$f(\\lambda x + (1 - \\lambda)y) \\leq \\lambda f(x) + (1 - \\lambda)f(y)$$\n",
        "Si $f$ es dos veces diferenciable ($f \\in C^2$), la convexidad implica $f''(x) \\geq 0$ en todo el dominio. Implicación: Cualquier mínimo local encontrado en una función convexa es garantizado ser un Mínimo Global.\n",
        "\n",
        "**Definición 2.3 (Concavidad).** Una función $f$ es cóncava si el segmento de línea que une dos puntos cualesquiera queda por debajo de la curva (forma de colina o tazón invertido). Si $f \\in C^2$, implica $f''(x) \\leq 0$. Implicación: Cualquier máximo local es un Máximo Global."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.1.1. Implementación Computacional: Visualización de Dualidad\nEl siguiente script en Python utiliza matplotlib para graficar ambas topologías, permitiendo visualizar la relación entre el signo de la segunda derivada y la forma de la función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Definir rango de evaluacion\n",
        "x = np.linspace(-10, 10, 400)\n",
        "\n",
        "# Funcion Convexa (Tazon) -> f''(x) > 0 -> Para Minimizar\n",
        "# Ejemplo: Energia Potencial\n",
        "f_convex = x**2\n",
        "\n",
        "# Funcion Concava (Colina) -> f''(x) < 0 -> Para Maximizar\n",
        "# Ejemplo: Curva de Rendimiento\n",
        "f_concave = -x**2 + 100\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Grafica 1: Convexidad\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(x, f_convex, 'b', label=r'Convexa ($x^2$)')\n",
        "plt.title('Convexidad (Busqueda de Minimo)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Grafica 2: Concavidad\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x, f_concave, 'r', label=r'Concava ($-x^2 + 100$)')\n",
        "plt.title('Concavidad (Busqueda de Maximo)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Caracterización de Óptimos Locales\n",
        "\n",
        "Para funciones suaves, el cálculo diferencial nos ofrece herramientas poderosas mediante la expansión de Taylor alrededor de un punto $x^*$:\n",
        "$$f(x) = f(x^*) + f'(x^*)(x - x^*) + \\frac{1}{2}f''(x^*)(x - x^*)^2 + O((x - x^*)^3)$$\n",
        "\n",
        "1. **Condición Necesaria (Primer Orden):** Si $x^*$ es un óptimo local, entonces $f'(x^*) = 0$ (punto estacionario).\n",
        "2. **Condición Suficiente (Segundo Orden):**\n",
        "   - Si $f'(x^*) = 0$ y $f''(x^*) > 0$, entonces $x^*$ es un Mínimo Local Estricto.\n",
        "   - Si $f'(x^*) = 0$ y $f''(x^*) < 0$, entonces $x^*$ es un Máximo Local Estricto.\n",
        "   - Si $f''(x^*) = 0$, la prueba no es concluyente (puede ser punto de inflexión)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Métodos de Búsqueda sin Derivadas\n",
        "\n",
        "> **Nota de Cátedra:** En la práctica ingenieril, muchas veces no tenemos una fórmula bonita $f(x)$. Tenemos una simulación de “caja negra” (ej. Aspen HYSYS, ANSYS). En estos casos, no podemos calcular derivadas analíticas ($f'(x)$). Recurrimos a métodos geométricos.\n",
        "\n",
        "#### 2.3.1. Método de la Sección Dorada\n",
        "Es un método de reducción de intervalos que no requiere derivadas. Se basa en reducir el intervalo de incertidumbre $[a, b]$ evaluando la función en dos puntos intermedios determinados por la razón áurea $\\phi = \\frac{1+\\sqrt{5}}{2} \\approx 1.618$.\n",
        "\n",
        "**Ventaja:** Robusto. Garantiza convergencia si la función es unimodal.\n",
        "**Tasa de Convergencia:** Lineal (reducción del intervalo por factor 0.618 en cada paso).\n",
        "\n",
        "**Ejemplo de Ingeniería 2.1 (Optimización de Reactores Químicos).** Se desea encontrar la temperatura $T$ que maximiza el rendimiento $R(T)$ de una reacción. La función $R(T)$ proviene de una simulación compleja. La Sección Dorada acota el rango sistemáticamente sin necesitar gradientes termodinámicos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Callable, Tuple, List\n",
        "\n",
        "def golden_section_search_visual(\n",
        "    f: Callable[[float], float],\n",
        "    a: float,\n",
        "    b: float,\n",
        "    tol: float = 1e-5\n",
        ") -> Tuple[float, int, List[Tuple[float, float]]]:\n",
        "    \"\"\"\n",
        "    Encuentra el mínimo de f en [a , b ] usando la Sección Dorada .\n",
        "    Retorna : ( punto_minimo , iteraciones , historial_intervalos )\n",
        "    \"\"\"\n",
        "    # La razón áurea ( phi ) inversa : ( sqrt (5) -1) /2 approx 0.618\n",
        "    ratio = (math.sqrt(5) - 1) / 2\n",
        "    \n",
        "    # Inicialización\n",
        "    x1 = b - ratio * (b - a)\n",
        "    x2 = a + ratio * (b - a)\n",
        "    f1, f2 = f(x1), f(x2)\n",
        "    \n",
        "    # Guardamos la historia : (a , b )\n",
        "    history = [(a, b)]\n",
        "    iters = 0\n",
        "    \n",
        "    while (b - a) > tol:\n",
        "        iters += 1\n",
        "        if f1 < f2:\n",
        "            # El mínimo está en el sub-intervalo izquierdo\n",
        "            b = x2\n",
        "            x2 = x1\n",
        "            f2 = f1\n",
        "            x1 = b - ratio * (b - a)\n",
        "            f1 = f(x1)\n",
        "        else:\n",
        "            # El mínimo está en el sub-intervalo derecho\n",
        "            a = x1\n",
        "            x1 = x2\n",
        "            f1 = f2\n",
        "            x2 = a + ratio * (b - a)\n",
        "            f2 = f(x2)\n",
        "            \n",
        "        history.append((a, b))\n",
        "        \n",
        "    return (a + b) / 2, iters, history\n",
        "\n",
        "def plot_results(f, a_orig, b_orig, min_x, history):\n",
        "    \"\"\" Genera gráficos explicativos del proceso de optimización \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # --- GRÁFICO 1: La Función y el Mínimo ---\n",
        "    x = np.linspace(a_orig, b_orig, 400)\n",
        "    y = [f(val) for val in x]\n",
        "    \n",
        "    ax1.plot(x, y, label='f(x)', color='blue', alpha=0.6)\n",
        "    ax1.scatter(min_x, f(min_x), color='red', zorder=5, s=100,\n",
        "                label=f'Mínimo: {min_x:.4f}')\n",
        "    ax1.set_title('Función Objetivo y Mínimo Encontrado')\n",
        "    ax1.set_xlabel('x')\n",
        "    ax1.set_ylabel('f(x)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "    \n",
        "    # --- GRÁFICO 2: Reducción del Intervalo (El \"Embudo\") ---\n",
        "    iterations = range(len(history))\n",
        "    # Calculamos el centro y el ancho de cada intervalo para graficarlo\n",
        "    centers = [(h[0] + h[1])/2 for h in history]\n",
        "    errors = [(h[1] - h[0])/2 for h in history]\n",
        "    \n",
        "    ax2.errorbar(centers, iterations, xerr=errors, fmt='o',\n",
        "                 capsize=3, color='green', ecolor='orange', elinewidth=3)\n",
        "    \n",
        "    ax2.invert_yaxis()\n",
        "    ax2.set_title(f'Convergencia del Intervalo (Sección Dorada)')\n",
        "    ax2.set_xlabel('Espacio de Búsqueda (x)')\n",
        "    ax2.set_ylabel('Número de Iteración')\n",
        "    ax2.axvline(x=min_x, color='red', linestyle='--', alpha=0.3,\n",
        "                label='Mínimo Real')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- EJECUCIÓN ---\n",
        "# 1. Definimos la función lambda\n",
        "func = lambda x: (x - 4)**2 + 10\n",
        "\n",
        "# 2. Definimos el intervalo inicial\n",
        "inicio, fin = 0, 10\n",
        "\n",
        "# 3. Ejecutamos el algoritmo\n",
        "resultado, n_iters, historial = golden_section_search_visual(func, inicio, fin)\n",
        "\n",
        "print(f\"--> Óptimo encontrado en x = {resultado:.5f}\")\n",
        "print(f\"--> Precisión alcanzada tras {n_iters} iteraciones\")\n",
        "print(\"--> Generando gráficos...\")\n",
        "\n",
        "# 4. Graficamos\n",
        "plot_results(func, inicio, fin, resultado, historial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.3.2. Interpolación Parabólica\n",
        "Este método aprovecha que muchas funciones suaves se comportan cuadráticamente cerca del óptimo. Dados tres puntos $(x_1, f_1), (x_2, f_2), (x_3, f_3)$, se ajusta un polinomio de segundo grado $q(x)$. El mínimo de este polinomio se usa como la nueva estimación del óptimo.\n",
        "\n",
        "$$x_{new} = x_2 - \\frac{1}{2} \\frac{(x_2 - x_1)^2(f_2 - f_3) - (x_2 - x_3)^2(f_2 - f_1)}{(x_2 - x_1)(f_2 - f_3) - (x_2 - x_3)(f_2 - f_1)}$$\n",
        "\n",
        "Posee una tasa de convergencia superlineal (orden $\\approx 1.324$), siendo más rápida que la Sección Dorada cerca del óptimo, pero menos robusta lejos de él."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. El Problema Multivariable: Fundamentos Matemáticos\n",
        "Aquí $f : \\mathbb{R}^n \\to \\mathbb{R}$. La variable de diseño es un vector $x = [x_1, x_2, ..., x_n]^T$.\n",
        "\n",
        "### 3.1. El Gradiente y la Hessiana\n",
        "\n",
        "> **Nota de Cátedra:** Deben visualizar el Gradiente como su brújula (indica la dirección) y la Hessiana como el mapa de relieve (indica la curvatura del terreno). Sin estas dos definiciones, estamos ciegos en un espacio n-dimensional. En Python, utilizamos la librería `sympy` para cálculo simbólico exacto.\n",
        "\n",
        "**Definición 3.1 (Vector Gradiente).** Es el vector de primeras derivadas parciales. Físicamente, apunta en la dirección de máximo crecimiento de la función.\n",
        "$$\\nabla f(x) = \\left[ \\frac{\\partial f}{\\partial x_1}, ..., \\frac{\\partial f}{\\partial x_n} \\right]^T$$\n",
        "En el óptimo sin restricciones, el gradiente debe ser nulo ($\\nabla f = 0$).\n",
        "\n",
        "**Definición 3.2 (Matriz Hessiana).** Es la matriz simétrica de $n \\times n$ de segundas derivadas parciales.\n",
        "$$H_{ij}(x) = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$$\n",
        "Proporciona información sobre la curvatura de la función y es crucial para determinar si un punto estacionario es máximo, mínimo o punto de silla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sympy as sp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def analyze_and_plot_multivar(expr_str):\n",
        "    \"\"\"\n",
        "    Analiza una función de dos variables, encuentra su óptimo y genera gráficos 3D/2D.\n",
        "    \"\"\"\n",
        "    # 1. Configuración Simbólica\n",
        "    x, y = sp.symbols('x y')\n",
        "    f = sp.sympify(expr_str)\n",
        "    \n",
        "    # 2. Cálculo del Gradiente (Primeras derivadas)\n",
        "    # El gradiente es un vector : [df/dx , df/dy]\n",
        "    grad_f = sp.Matrix([sp.diff(f, var) for var in (x, y)])\n",
        "    \n",
        "    # 3. Cálculo de la Hessiana (Segundas derivadas)\n",
        "    hessian_f = sp.hessian(f, (x, y))\n",
        "    \n",
        "    # 4. Encontrar Puntos Críticos (Donde el gradiente es cero)\n",
        "    critical_points = sp.solve(grad_f, (x, y))\n",
        "    \n",
        "    # Nota: sp.solve puede devolver un diccionario o una lista de tuplas\n",
        "    if isinstance(critical_points, dict):\n",
        "        crit_x, crit_y = float(critical_points[x]), float(critical_points[y])\n",
        "    else:\n",
        "        # Tomamos el primer punto encontrado para este ejemplo\n",
        "        crit_x, crit_y = float(critical_points[0][0]), float(critical_points[0][1])\n",
        "        \n",
        "    # 5. Evaluar Hessiana en el punto crítico para ver concavidad\n",
        "    H_val = hessian_f.subs({x: crit_x, y: crit_y})\n",
        "    eigenvals = H_val.eigenvals() # Clave para saber si es min/max\n",
        "    \n",
        "    print(f\"--- ANÁLISIS ---\")\n",
        "    print(f\"Función: f(x, y) = {f}\")\n",
        "    print(f\"Gradiente: {grad_f.T}\")\n",
        "    print(f\"Hessiana:\\n{hessian_f}\")\n",
        "    print(f\"Punto Crítico encontrado: ({crit_x}, {crit_y})\")\n",
        "    print(f\"Autovalores de la Hessiana: {list(eigenvals.keys())}\")\n",
        "    \n",
        "    # --- VISUALIZACIÓN ---\n",
        "    \n",
        "    # Convertir función simbólica a función numérica de Python (lambdify)\n",
        "    f_num = sp.lambdify((x, y), f, 'numpy')\n",
        "    \n",
        "    # Crear rejilla de datos (Grid) alrededor del punto crítico\n",
        "    range_val = 5\n",
        "    x_vals = np.linspace(crit_x - range_val, crit_x + range_val, 100)\n",
        "    y_vals = np.linspace(crit_y - range_val, crit_y + range_val, 100)\n",
        "    X, Y = np.meshgrid(x_vals, y_vals)\n",
        "    Z = f_num(X, Y)\n",
        "    \n",
        "    fig = plt.figure(figsize=(16, 7))\n",
        "    \n",
        "    # GRÁFICO 1: Superficie 3D\n",
        "    ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\n",
        "    ax1.scatter(crit_x, crit_y, f_num(crit_x, crit_y), color='red', s=100, label='Punto Crítico')\n",
        "    ax1.set_title('Superficie 3D: f(x, y)')\n",
        "    ax1.set_xlabel('x')\n",
        "    ax1.set_ylabel('y')\n",
        "    ax1.set_zlabel('f(x, y)')\n",
        "    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=5)\n",
        "    ax1.view_init(elev=30, azim=45)\n",
        "    \n",
        "    # GRÁFICO 2: Mapa de Contorno (Vista superior)\n",
        "    ax2 = fig.add_subplot(1, 2, 2)\n",
        "    contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\n",
        "    ax2.scatter(crit_x, crit_y, color='red', s=100, marker='x', label=f'Mínimo ({crit_x},{crit_y})')\n",
        "    \n",
        "    # Opcional : Agregar flechas de gradiente (Quiver plot)\n",
        "    grad_x_func = sp.lambdify((x, y), grad_f[0], 'numpy')\n",
        "    grad_y_func = sp.lambdify((x, y), grad_f[1], 'numpy')\n",
        "    skip = (slice(None, None, 10), slice(None, None, 10))\n",
        "    U = grad_x_func(X, Y)\n",
        "    V = grad_y_func(X, Y)\n",
        "    \n",
        "    # Negativo del gradiente apunta al mínimo\n",
        "    ax2.quiver(X[skip], Y[skip], -U[skip], -V[skip], color='white', alpha=0.5)\n",
        "    \n",
        "    ax2.set_title('Curvas de Nivel y Dirección de Descenso')\n",
        "    ax2.set_xlabel('x')\n",
        "    ax2.set_ylabel('y')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Ejecución\n",
        "# Usamos tu ejemplo: x^2 + y^2 + x*y\n",
        "expr = \"x**2 + y**2 + x*y\"\n",
        "analyze_and_plot_multivar(expr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2. Teorema de Taylor Multivariable\n",
        "Para entender los algoritmos de optimización avanzados (como Newton), debemos comprender la aproximación local de segundo orden. Sea $f : \\mathbb{R}^n \\to \\mathbb{R}$ con derivadas continuas hasta segundo orden. La expansión alrededor de $x$ en dirección $h$ es:\n",
        "$$f(x + h) = f(x) + \\nabla f(x)^T h + \\frac{1}{2}h^T H(x)h + o(\\lVert h \\rVert^2)$$\n",
        "Esta ecuación justifica que si el gradiente es cero ($\\nabla f = 0$), el comportamiento de la función está dominado enteramente por el término cuadrático $\\frac{1}{2}h^T H h$.\n",
        "\n",
        "### 3.3. Formas Cuadráticas y Criterio de Sylvester\n",
        "Para asegurar que un punto estacionario es un **Mínimo Estricto**, la Hessiana $H$ debe ser **Definida Positiva**. Esto significa que $h^T H h > 0$ para todo vector no nulo $h$.\n",
        "\n",
        "**Teorema 3.1 (Criterio de Sylvester).** Una matriz simétrica $H$ es definida positiva si y solo si todos sus Menores Principales Dominantes ($\\Delta_k$) son estrictamente positivos.\n",
        "\n",
        "Para una matriz $3 \\times 3$:\n",
        "$$ H = \\begin{bmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{bmatrix} $$\n",
        "1. $\\Delta_1 = h_{11} > 0$\n",
        "2. $\\Delta_2 = \\det \\begin{pmatrix} h_{11} & h_{12} \\\\ h_{21} & h_{22} \\end{pmatrix} > 0$\n",
        "3. $\\Delta_3 = \\det(H) > 0$\n",
        "\n",
        "Si se cumple, la función tiene forma de “tazón” y el punto es un mínimo. Si los signos alternan ($\\Delta_1 < 0, \\Delta_2 > 0, \\Delta_3 < 0$), la matriz es definida negativa (Máximo).\n",
        "\n",
        "### 3.4. Gráficos de Contorno\n",
        "Un gráfico de contorno representa curvas de nivel $C_k = \\{x \\in \\mathbb{R}^2 | f(x) = k\\}$. Son la proyección 2D de la superficie 3D. Propiedad fundamental: El vector gradiente $\\nabla f(x)$ es siempre **ortogonal** (perpendicular) a la curva de nivel en el punto $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Algoritmos de Descenso Multivariable\n",
        "\n",
        "Casi todos los algoritmos de optimización iterativa multivariable siguen la misma estructura canónica:\n",
        "$$x_{k+1} = x_k + \\alpha_k d_k$$\n",
        "Donde $d_k$ es la dirección de búsqueda y $\\alpha_k$ es el tamaño de paso.\n",
        "\n",
        "### 4.1. Análisis de Convergencia\n",
        "> **Nota de Cátedra:** En ingeniería de control o sistemas en tiempo real, no basta con encontrar la solución; importa qué tan rápido llegamos a ella.\n",
        "\n",
        "Se dice que la convergencia es de orden $p$ si existe $C > 0$ tal que $\\lim_{k\\to\\infty} \\frac{|e_{k+1}|}{|e_k|^p} = C$, donde $e_k$ es el error.\n",
        "\n",
        "- **Convergencia Lineal ($p = 1$):** El error se reduce en una fracción constante. (Ej. Gradiente Descendente). Robusta pero lenta.\n",
        "- **Convergencia Cuadrática ($p = 2$):** El número de cifras significativas correctas se duplica en cada iteración. (Ej. Método de Newton). Extremadamente rápida cerca del óptimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2. Método del Máximo Descenso (Gradient Descent)\n",
        "Se elige $d_k = -\\nabla f(x_k)$. Basado en que el negativo del gradiente es la dirección de descenso más rápido localmente.\n",
        "\n",
        "**Problema:** Genera trayectorias en “zigzag” en valles estrechos (funciones mal condicionadas), haciendo la convergencia muy lenta.\n",
        "**Uso:** Entrenamiento de Redes Neuronales (Deep Learning)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def f(x):\n",
        "    \"\"\" Función objetivo : f(x, y) = x^2 + 5y^2 (Elipse alargada / Valle estrecho) \"\"\"\n",
        "    return x[0]**2 + 5*x[1]**2\n",
        "\n",
        "def grad_f(x):\n",
        "    \"\"\" Gradiente analítico : f = [2x, 10y] \"\"\"\n",
        "    return np.array([2*x[0], 10*x[1]])\n",
        "\n",
        "def gradient_descent(start_point, learning_rate=0.05, n_iters=50, tol=1e-6):\n",
        "    \"\"\"\n",
        "    Ejecuta el descenso del gradiente con historial y criterio de parada.\n",
        "    \"\"\"\n",
        "    path = [start_point]\n",
        "    x_curr = start_point.copy()\n",
        "    \n",
        "    for i in range(n_iters):\n",
        "        grad = grad_f(x_curr)\n",
        "        \n",
        "        # Criterio de parada: Si la norma del gradiente es casi 0, ya llegamos.\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            print(f\"Convergió en la iteración {i}\")\n",
        "            break\n",
        "            \n",
        "        # Paso de actualización\n",
        "        x_curr = x_curr - learning_rate * grad\n",
        "        path.append(x_curr)\n",
        "        \n",
        "    return np.array(path)\n",
        "\n",
        "# --- PARÁMETROS ---\n",
        "x_start = np.array([8.0, 4.0])\n",
        "lr = 0.05 # Tasa de aprendizaje\n",
        "iterations = 50\n",
        "\n",
        "# Ejecutar optimización\n",
        "path = gradient_descent(x_start, learning_rate=lr, n_iters=iterations)\n",
        "\n",
        "# --- VISUALIZACIÓN ---\n",
        "# Crear rejilla para el gráfico\n",
        "x_grid = np.linspace(-10, 10, 200)\n",
        "y_grid = np.linspace(-5, 5, 200)\n",
        "X, Y = np.meshgrid(x_grid, y_grid)\n",
        "Z = f([X, Y]) # Aprovechamos broadcasting de numpy\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# 1. Mapa de colores de fondo (Topografía)\n",
        "plt.contourf(X, Y, Z, levels=30, cmap='coolwarm', alpha=0.6)\n",
        "\n",
        "# 2. Líneas de contorno (Curvas de nivel)\n",
        "contours = plt.contour(X, Y, Z, levels=20, colors='black', linewidths=0.5, alpha=0.8)\n",
        "plt.clabel(contours, inline=True, fontsize=8)\n",
        "\n",
        "# 3. Trayectoria del algoritmo\n",
        "plt.plot(path[:,0], path[:,1], color='white', linewidth=2, linestyle='--', label='Camino de Descenso')\n",
        "plt.scatter(path[:,0], path[:,1], color='red', s=30, zorder=5) # Puntos intermedios\n",
        "\n",
        "# 4. Inicio y Fin destacados\n",
        "plt.scatter(x_start[0], x_start[1], color='lime', s=150, edgecolors='black', label='Inicio', zorder=10)\n",
        "plt.scatter(path[-1,0], path[-1,1], color='yellow', marker='*', s=250, edgecolors='black', label='Óptimo Local', zorder=10)\n",
        "\n",
        "# --- MEJORA CRÍTICA ---\n",
        "# Esto asegura que 1 unidad en X sea visualmente igual a 1 unidad en Y.\n",
        "# Sin esto, la ortogonalidad no se ve correctamente.\n",
        "plt.axis('equal')\n",
        "\n",
        "plt.title(f'Descenso del Gradiente: Zig-Zag en Valles Estrechos\\nLearning Rate: {lr} | Pasos: {len(path)-1}')\n",
        "plt.xlabel('$x_1$')\n",
        "plt.ylabel('$x_2$')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=':', alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3. Método de Newton Multivariable\n",
        "\n",
        "Se elige $d_k = -[H(x_k)]^{-1} \\nabla f(x_k)$. Minimiza la aproximación cuadrática exacta de la función en cada paso. Transforma el espacio para que los contornos elípticos se vuelvan circulares, apuntando directamente al mínimo.\n",
        "\n",
        "**Ventaja:** Convergencia Cuadrática.\n",
        "**Desventaja:** Calcular la inversa de la Hessiana $H^{-1}$ es computacionalmente costoso ($O(n^3)$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import norm, solve\n",
        "\n",
        "# --- DEFINICIÓN DEL PROBLEMA ---\n",
        "def f(x):\n",
        "    \"\"\" Función objetivo : f(x, y) = x^2 + 5y^2 \"\"\"\n",
        "    return x[0]**2 + 5*x[1]**2\n",
        "\n",
        "def grad_f(x):\n",
        "    \"\"\" Gradiente : f = [2x, 10y] \"\"\"\n",
        "    return np.array([2*x[0], 10*x[1]])\n",
        "\n",
        "def hessian_f(x):\n",
        "    \"\"\" Hessiana : H = [[2, 0], [0, 10]] (Constante en cuadráticas) \"\"\"\n",
        "    return np.array([[2, 0],\n",
        "                     [0, 10]])\n",
        "\n",
        "# --- ALGORITMOS ---\n",
        "\n",
        "def gradient_descent(start_point, lr=0.05, steps=20):\n",
        "    \"\"\" Descenso de Gradiente (para comparación) \"\"\"\n",
        "    path = [start_point]\n",
        "    x = np.array(start_point)\n",
        "    for _ in range(steps):\n",
        "        g = grad_f(x)\n",
        "        x = x - lr * g\n",
        "        path.append(x)\n",
        "    return np.array(path)\n",
        "\n",
        "def newton_method(start_point, tol=1e-6, max_iter=10):\n",
        "    \"\"\" Método de Newton \"\"\"\n",
        "    path = [start_point]\n",
        "    x = np.array(start_point)\n",
        "    \n",
        "    for i in range(max_iter):\n",
        "        g = grad_f(x)\n",
        "        H = hessian_f(x)\n",
        "        \n",
        "        # Criterio de parada\n",
        "        if norm(g) < tol:\n",
        "            print(f\"Newton: Convergencia alcanzada en iteración {i}\")\n",
        "            break\n",
        "            \n",
        "        # Paso de Newton : H * direction = -g\n",
        "        # Resolvemos el sistema lineal en lugar de invertir la matriz (más eficiente)\n",
        "        direction = solve(H, -g)\n",
        "        x = x + direction\n",
        "        path.append(x)\n",
        "        \n",
        "    return np.array(path)\n",
        "\n",
        "# --- EJECUCIÓN ---\n",
        "start = [8.0, 4.0]\n",
        "\n",
        "# 1. Corremos Gradient Descent (el \"lento\")\n",
        "path_gd = gradient_descent(start, lr=0.05, steps=20)\n",
        "\n",
        "# 2. Corremos Newton (el \"rápido\")\n",
        "path_newton = newton_method(start)\n",
        "\n",
        "print(f\"Pasos Gradiente: {len(path_gd)-1}\")\n",
        "print(f\"Pasos Newton: {len(path_newton)-1}\")\n",
        "\n",
        "# --- VISUALIZACIÓN ---\n",
        "x_grid = np.linspace(-10, 10, 200)\n",
        "y_grid = np.linspace(-5, 5, 200)\n",
        "X, Y = np.meshgrid(x_grid, y_grid)\n",
        "Z = f([X, Y])\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Contornos\n",
        "plt.contourf(X, Y, Z, levels=30, cmap='gray_r', alpha=0.4)\n",
        "plt.contour(X, Y, Z, levels=20, colors='black', linewidths=0.5, alpha=0.5)\n",
        "\n",
        "# Trayectoria Gradiente (Rojo)\n",
        "plt.plot(path_gd[:,0], path_gd[:,1], 'r--o', label='Gradiente (Primer Orden)', markersize=4)\n",
        "\n",
        "# Trayectoria Newton (Azul)\n",
        "plt.plot(path_newton[:,0], path_newton[:,1], 'b-o', linewidth=3, label='Newton (Segundo Orden)', markersize=8)\n",
        "\n",
        "# Puntos clave\n",
        "plt.scatter(start[0], start[1], c='lime', s=150, edgecolors='black', zorder=10, label='Inicio')\n",
        "plt.scatter(0, 0, c='yellow', marker='*', s=200, edgecolors='black', zorder=10, label='Óptimo')\n",
        "\n",
        "plt.title('Comparación: Gradiente vs. Newton \\n (El poder de la Curvatura)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.axis('equal')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis de Convergencia: Gradiente vs. Newton\n",
        "\n",
        "Si observamos el gráfico generado, notaremos comportamientos radicalmente distintos:\n",
        "\n",
        "1. **Línea Roja (Gradiente):** Va \"tanteando\" el camino. Como la elipse es muy alargada, el gradiente le indica que baje muy rápido en el eje Y, pero avanza muy poco en el eje X, provocando el característico efecto de zig-zag. Esto sucede porque el método solo utiliza la pendiente (información de la primera derivada).\n",
        "2. **Línea Azul (Newton):** Va directo al centro en un solo salto.\n",
        "\n",
        "**Razón Matemática:**\n",
        "El método de Newton intenta aproximar la función localmente mediante una parábola (aproximación cuadrática) y salta directamente al mínimo de esa parábola teórica. Dado que tu función original $f(x)$ **YA ES** una parábola (una función cuadrática perfecta), la aproximación es exacta, y el cálculo matemático lleva al mínimo global de manera instantánea.\n",
        "\n",
        "**El “Truco” de Newton:**\n",
        "La regla de actualización se define formalmente como $x_{new} = x - H^{-1} \\nabla f$. La matriz Hessiana $H$ contiene la información de la curvatura (nos dice \"qué tan alargada es la elipse\" en cada dirección). Al multiplicar el gradiente por la inversa de la Hessiana ($H^{-1}$), el algoritmo esencialmente \"normaliza\" el terreno. Desde su perspectiva matemática, convierte la elipse distorsionada en un círculo perfecto, permitiéndole ignorar la distorsión y dar el paso directo hacia el óptimo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. El Problema del Valle Curvo\n",
        "\n",
        "La función de Rosenbrock, conocida como la \"Función Banana\", se define como:\n",
        "$$f(x, y) = (1 - x)^2 + 100(y - x^2)^2$$\n",
        "Su topología presenta un desafío único: un valle estrecho, plano y con forma parabólica. El mínimo global se encuentra en $(1, 1)$.\n",
        "\n",
        "### 5.1. Por qué falla el Gradiente (Primer Orden)\n",
        "El método de Descenso de Gradiente utiliza la actualización: $x_{new} = x - \\alpha \\nabla f(x)$.\n",
        "En el valle de Rosenbrock, las paredes son muy empinadas (alta pendiente en $y$) pero el fondo es casi plano (baja pendiente a lo largo de la curva).\n",
        "\n",
        "- El gradiente está dominado por la pendiente de las paredes.\n",
        "- **Resultado:** El algoritmo \"rebota\" de una pared a otra (zig-zag) y avanza muy lentamente hacia el mínimo, ignorando la curvatura del valle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import norm, solve\n",
        "\n",
        "# --- 1. DEFINICIÓN DE LA FUNCIÓN DE ROSENBROCK ---\n",
        "# f(x, y) = (1 - x)^2 + 100*(y - x^2)^2\n",
        "# Mínimo global en (1, 1)\n",
        "\n",
        "def f(x):\n",
        "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
        "\n",
        "def grad_f(x):\n",
        "    # Derivadas parciales\n",
        "    df_dx = -2 * (1 - x[0]) - 400 * x[0] * (x[1] - x[0]**2)\n",
        "    df_dy = 200 * (x[1] - x[0]**2)\n",
        "    return np.array([df_dx, df_dy])\n",
        "\n",
        "def hessian_f(x):\n",
        "    # Matriz de segundas derivadas (Hessiana)\n",
        "    # Cambia en cada punto (x, y), ya no es constante\n",
        "    d2f_dx2 = 2 - 400 * x[1] + 1200 * x[0]**2\n",
        "    d2f_dxdy = -400 * x[0]\n",
        "    d2f_dy2 = 200\n",
        "    return np.array([[d2f_dx2, d2f_dxdy],\n",
        "                     [d2f_dxdy, d2f_dy2]])\n",
        "\n",
        "# --- 2. ALGORITMOS ---\n",
        "\n",
        "def gradient_descent(start, lr=0.002, max_iter=2000):\n",
        "    path = [start]\n",
        "    x = np.array(start)\n",
        "    for _ in range(max_iter):\n",
        "        g = grad_f(x)\n",
        "        # Nota : Usamos un Learning Rate muy pequeño porque Rosenbrock es \"peligrosa\"\n",
        "        x = x - lr * g\n",
        "        path.append(x)\n",
        "        if norm(g) < 1e-4: break\n",
        "    return np.array(path)\n",
        "\n",
        "def newton_method(start, max_iter=100):\n",
        "    path = [start]\n",
        "    x = np.array(start)\n",
        "    for i in range(max_iter):\n",
        "        g = grad_f(x)\n",
        "        H = hessian_f(x)\n",
        "        \n",
        "        if norm(g) < 1e-6:\n",
        "            print(f\"Newton convergió en iteración : {i}\")\n",
        "            break\n",
        "            \n",
        "        # Paso de Newton : x_new = x - H^(-1) * g\n",
        "        try:\n",
        "            direction = solve(H, -g)\n",
        "            x = x + direction # Learning rate de 1.0 (Full Newton)\n",
        "            path.append(x)\n",
        "        except np.linalg.LinAlgError:\n",
        "            print(\"Matriz singular encontrada, deteniendo Newton.\")\n",
        "            break\n",
        "            \n",
        "    return np.array(path)\n",
        "\n",
        "# --- 3. EJECUCIÓN ---\n",
        "# Punto de inicio clásico lejos del óptimo (1,1)\n",
        "start_point = [-1.5, 2.0]\n",
        "\n",
        "# Ejecutar Gradiente (necesita muchas iteraciones)\n",
        "path_gd = gradient_descent(start_point, lr=0.0015, max_iter=5000)\n",
        "\n",
        "# Ejecutar Newton\n",
        "path_newton = newton_method(start_point)\n",
        "\n",
        "print(f\"Pasos Gradiente: {len(path_gd)} (Apenas avanza)\")\n",
        "print(f\"Pasos Newton: {len(path_newton)} (Llega al destino)\")\n",
        "\n",
        "# --- 4. VISUALIZACIÓN ---\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Generar Grid para el mapa de calor\n",
        "x_vals = np.linspace(-2, 2, 200)\n",
        "y_vals = np.linspace(-1, 3, 200)\n",
        "X, Y = np.meshgrid(x_vals, y_vals)\n",
        "Z = (1 - X)**2 + 100 * (Y - X**2)**2\n",
        "\n",
        "# Usamos escala logarítmica en los contornos para ver mejor el valle plano\n",
        "plt.contourf(X, Y, Z, levels=np.logspace(-1, 3, 30), cmap='gray_r', alpha=0.5)\n",
        "plt.contour(X, Y, Z, levels=np.logspace(-1, 3, 30), colors='black', linewidths=0.3, alpha=0.5)\n",
        "\n",
        "# Plot Trayectorias\n",
        "plt.plot(path_gd[:,0], path_gd[:,1], 'r-', linewidth=2, label='Gradiente (Se atasca en el valle)')\n",
        "plt.plot(path_newton[:,0], path_newton[:,1], 'b-o', linewidth=2, label='Newton (Sigue la curva)')\n",
        "\n",
        "# Puntos Inicio y Fin\n",
        "plt.scatter(start_point[0], start_point[1], color='lime', s=100, edgecolors='black', label='Inicio', zorder=10)\n",
        "plt.scatter(1, 1, color='yellow', marker='*', s=300, edgecolors='black', label='Mínimo Global (1,1)', zorder=10)\n",
        "\n",
        "plt.title('Rosenbrock (\"Banana\") Function : Newton vs Gradiente')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.2)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. La Solución de Newton (Segundo Orden)\n",
        "\n",
        "El método de Newton introduce la matriz Hessiana $H_f$ en la ecuación de actualización:\n",
        "$$x_{new} = x - H_f^{-1}(x) \\nabla f(x)$$\n",
        "\n",
        "### 6.1. ¿Qué hace la Hessiana realmente?\n",
        "Matemáticamente, la Hessiana $H_f$ contiene información sobre la curvatura de la función en todas las direcciones. Al multiplicar el gradiente por la inversa de la Hessiana ($H^{-1}$), ocurren dos fenómenos geométricos vitales:\n",
        "\n",
        "1. **Re-escalado (Normalización):** La Hessiana detecta que la curvatura en la dirección $y$ es enorme (paredes empinadas) y la curvatura a lo largo del valle es pequeña. La inversa $H^{-1}$ \"divide\" por esta curvatura. Esto reduce el paso en direcciones empinadas (evitando el rebote) y agranda el paso en direcciones planas (acelerando el avance).\n",
        "2. **Rotación del Vector:** Ajusta la dirección para seguir la curva óptima en lugar de solo la pendiente local."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}